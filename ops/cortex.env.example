#
# Copy to /etc/cortex/cortex.env (systemd) or ops/.env (compose) and fill in secrets.
# Use a secret manager (Vault/SSM/SealedSecrets) or an untracked env file for production.
# Never commit populated secrets.
#

# Core
CORTEX_ENV=strix
CORTEX_SKIP_AUTH=false  # forced false in strix/prod; only set true in local
CORTEX_AUTH_SECRET=     # REQUIRED: 32+ char random secret (do not use defaults)
CORTEX_ACCESS_TOKEN_MINUTES=15
CORTEX_REFRESH_TOKEN_DAYS=7
CORTEX_DOMAIN=your-frontend.example              # REQUIRED: public hostname for Caddy + frontend
CORTEX_ADMIN_EMAIL=admin@example.com             # REQUIRED: for ACME/Letâ€™s Encrypt contact
CORTEX_ALLOWED_ORIGINS=https://your-frontend.example

# Model root (host path) used by docker-compose.prod volume bind
MODELS_PATH=/data/cortex-models                   # Host path for full lane models (bind-mounted)
# Inference images (pin tags, no :latest)
CORTEX_VLLM_IMAGE=vllm-rocm-nix:0.12.0            # Pinned vLLM image tag
LLAMA_CPP_IMAGE=ghcr.io/ggerganov/llama.cpp:server-rocm-0.2.90  # Pinned llama.cpp tag

# Optional tiny models for smoke tests (download with ops/download_minimal_models.sh)
CORTEX_MINIMAL_VLLM_MODEL_PATH=/models/minimal/vllm/TinyLlama-1.1B-Chat-v1.0
CORTEX_MINIMAL_GGUF_MODEL_PATH=/models/minimal/gguf/TinyLlama-1.1B-Chat-v1.0.Q4_K_M.gguf

# Runtime guard
RUNNING_IN_DOCKER=0  # set to 1 when running inside Docker/Compose
# Uncomment for non-Nix systemd/bare-metal deployments:
# CORTEX_ALLOW_NON_NIX=1

# Database
CORTEX_DATABASE_URL=postgresql://USER:PASS@HOST:5432/cortex
POSTGRES_USER=cortex
POSTGRES_PASSWORD=

# Storage (choose one)
# Option A: Local persistent volume (default in compose)
# Storage (explicitly allow local durable volume in prod when backed up)
CORTEX_STORAGE_BACKEND=local
CORTEX_ALLOW_LOCAL_STORAGE=1
CORTEX_STORAGE_LOCAL_DIR=/app/storage_uploads
# Option B: S3/MinIO (set backend=s3 and provide bucket/endpoint/creds)
# CORTEX_STORAGE_BACKEND=s3
# CORTEX_STORAGE_BUCKET=cortex-ingest            # REQUIRED when backend=s3
# CORTEX_STORAGE_ENDPOINT_URL=https://minio.example
# CORTEX_STORAGE_ACCESS_KEY=                     # REQUIRED when backend=s3
# CORTEX_STORAGE_SECRET_KEY=                     # REQUIRED when backend=s3

# Services
CORTEX_QDRANT_URL=http://qdrant:6333
CORTEX_N8N_BASE_URL=http://n8n:5678
CORTEX_LLM_BASE_URL=http://inference-vllm:8000/v1
CORTEX_N8N_API_KEY=

# n8n UI credentials (use only when exposing the UI behind VPN/allowlist/auth proxy)
N8N_BASIC_AUTH_USER=        # REQUIRED: strong username
N8N_BASIC_AUTH_PASSWORD=    # REQUIRED: strong password
N8N_ENCRYPTION_KEY=         # REQUIRED: 32+ chars

# Exposure & rate limits (defaults are conservative; override per environment)
N8N_ALLOWED_IPS=10.0.0.0/8 172.16.0.0/12 192.168.0.0/16
CORTEX_RATE_LIMIT_EVENTS=300
CORTEX_RATE_LIMIT_WINDOW=1m
CORTEX_RATE_LIMIT_BURST=50
CORTEX_WEBHOOK_RATE_EVENTS=120
CORTEX_WEBHOOK_RATE_WINDOW=1m
CORTEX_WEBHOOK_RATE_BURST=30

# LLM backend defaults
CORTEX_LLM_BACKEND=local_http
CORTEX_LLM_DEFAULT_LANE=orchestrator

# Lane configs (set per deployment)
CORTEX_LANE_ORCHESTRATOR_URL=http://inference-vllm:8000/v1
CORTEX_LANE_ORCHESTRATOR_MODEL=Qwen3-30B-Thinking
CORTEX_LANE_ORCHESTRATOR_MODEL_PATH=/models/vllm/orchestrator/bf16
CORTEX_LANE_ORCHESTRATOR_BACKEND=vllm

CORTEX_LANE_CODER_URL=http://inference-vllm:8000/v1
CORTEX_LANE_CODER_MODEL=Qwen3-Coder-30B-1M
CORTEX_LANE_CODER_MODEL_PATH=/models/vllm/coder/bf16
CORTEX_LANE_CODER_BACKEND=vllm

CORTEX_LANE_FAST_RAG_URL=http://inference-vllm:8000/v1
CORTEX_LANE_FAST_RAG_MODEL=MegaBeam-Mistral-7B-512k
CORTEX_LANE_FAST_RAG_MODEL_PATH=/models/vllm/fast_rag/bf16
CORTEX_LANE_FAST_RAG_BACKEND=vllm

CORTEX_LANE_SUPER_READER_URL=http://llama-super-reader:8080/v1
CORTEX_LANE_SUPER_READER_MODEL=Nemotron-8B-UltraLong-4M
CORTEX_LANE_SUPER_READER_MODEL_PATH=/models/gguf/nemotron-8b-instruct.Q4_K_M.gguf
CORTEX_LANE_SUPER_READER_BACKEND=llama_cpp

CORTEX_LANE_GOVERNANCE_URL=http://llama-governance:8081/v1
CORTEX_LANE_GOVERNANCE_MODEL=Granite-4.x-Long-Context
CORTEX_LANE_GOVERNANCE_MODEL_PATH=/models/gguf/granite-3.0-8b-instruct.Q4_K_M.gguf
CORTEX_LANE_GOVERNANCE_BACKEND=llama_cpp

# ROCm / vLLM tuning
HIP_VISIBLE_DEVICES=0
HSA_OVERRIDE_GFX_VERSION=11.0.0
VLLM_TARGET_DEVICE=rocm
VLLM_ROCM_USE_AITER=1
VLLM_ROCM_USE_SKINNY_GEMM=1
GPU_MEM_UTIL=0.48
MAX_MODEL_LEN=32768

# Hugging Face cache (shared)
HF_HOME=/models/hf_cache
HF_TOKEN=    # REQUIRED for private/embedding models; also used at build time if provided

# Embeddings device (set to rocm/cuda/auto/cpu based on target hardware)
CORTEX_EMBEDDING_DEVICE=auto
