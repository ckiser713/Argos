# =============================================================================
# Cortex Production Docker Compose
# =============================================================================
# Full production stack with:
# - Caddy reverse proxy (auto-SSL)
# - PostgreSQL database
# - Qdrant vector database
# - vLLM inference (ROCm/Strix Halo)
# - llama.cpp for long-context models
# - n8n workflow automation
# - Backend (FastAPI)
# - Frontend (React, served via Caddy)
#
# Usage:
#   docker compose -f ops/docker-compose.prod.yml up -d
#
# Prerequisites:
#   - Copy ../.env.example to ../.env and configure
#   - Download models: ./download_all_models.sh
#   - Ensure /data/cortex-models exists with models
# =============================================================================

version: '3.8'

# Shared environment variables
x-common-env: &common-env
  CORTEX_ENV: ${CORTEX_ENV:-strix}
  CORTEX_AUTH_SECRET: ${CORTEX_AUTH_SECRET:?CORTEX_AUTH_SECRET is required}
  CORTEX_DATABASE_URL: postgresql://cortex:${POSTGRES_PASSWORD:?POSTGRES_PASSWORD is required}@postgres:5432/cortex
  CORTEX_QDRANT_URL: http://qdrant:6333
  CORTEX_N8N_BASE_URL: http://n8n:5678
  CORTEX_LLM_BASE_URL: http://inference-vllm:8000/v1
  HF_TOKEN: ${HF_TOKEN:-}

x-model-paths: &model-paths
  CORTEX_LANE_ORCHESTRATOR_MODEL_PATH: /models/vllm/orchestrator/bf16
  CORTEX_LANE_CODER_MODEL_PATH: /models/vllm/coder/bf16
  CORTEX_LANE_FAST_RAG_MODEL_PATH: /models/vllm/fast_rag/bf16
  CORTEX_LANE_SUPER_READER_MODEL_PATH: /models/gguf/nemotron-8b-instruct.Q4_K_M.gguf
  CORTEX_LANE_GOVERNANCE_MODEL_PATH: /models/gguf/granite-3.0-8b-instruct.Q4_K_M.gguf

x-healthcheck-defaults: &healthcheck-defaults
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 60s

x-restart-policy: &restart-policy
  restart: unless-stopped

services:
  # ===========================================================================
  # Reverse Proxy - Caddy with automatic HTTPS
  # ===========================================================================
  caddy:
    image: caddy:2-alpine
    container_name: cortex-caddy
    <<: *restart-policy
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"  # HTTP/3
    environment:
      CORTEX_DOMAIN: ${CORTEX_DOMAIN:-localhost}
      CORTEX_ADMIN_EMAIL: ${CORTEX_ADMIN_EMAIL:-admin@localhost}
    volumes:
      - ./caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
      - frontend_dist:/srv/frontend:ro
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - cortex-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/health"]
      <<: *healthcheck-defaults

  # ===========================================================================
  # Database - PostgreSQL
  # ===========================================================================
  postgres:
    image: postgres:15-alpine
    container_name: cortex-postgres
    <<: *restart-policy
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-cortex}
      POSTGRES_USER: ${POSTGRES_USER:-cortex}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD is required}
      # Performance tuning for Strix Halo (128GB RAM)
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C"
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=4GB"
      - "-c"
      - "effective_cache_size=12GB"
      - "-c"
      - "maintenance_work_mem=1GB"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=64MB"
      - "-c"
      - "default_statistics_target=100"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "work_mem=256MB"
      - "-c"
      - "min_wal_size=1GB"
      - "-c"
      - "max_wal_size=4GB"
      - "-c"
      - "max_connections=200"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d:ro
    networks:
      - cortex-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U cortex -d cortex"]
      <<: *healthcheck-defaults
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

  # ===========================================================================
  # Vector Database - Qdrant
  # ===========================================================================
  qdrant:
    image: qdrant/qdrant:latest
    container_name: cortex-qdrant
    <<: *restart-policy
    volumes:
      - qdrant_data:/qdrant/storage
      - qdrant_snapshots:/qdrant/snapshots
    networks:
      - cortex-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:6333/health"]
      <<: *healthcheck-defaults
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

  # ===========================================================================
  # LLM Inference - vLLM (ROCm/Strix Halo)
  # ===========================================================================
  inference-vllm:
    image: ${CORTEX_VLLM_IMAGE:-vllm-rocm-strix:latest}
    container_name: cortex-vllm
    <<: *restart-policy
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model "${CORTEX_LANE_ORCHESTRATOR_MODEL:-deepseek-ai/DeepSeek-R1-Distill-Qwen-32B}"
      --quantization fp8
      --kv-cache-dtype fp8
      --host 0.0.0.0
      --port 8000
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render
    environment:
      <<: *model-paths
      VLLM_GPU_MEMORY_UTILIZATION: ${VLLM_GPU_MEMORY_UTILIZATION:-0.45}
      VLLM_ENFORCE_EAGER: ${VLLM_ENFORCE_EAGER:-true}
      VLLM_MAX_MODEL_LEN: ${VLLM_MAX_MODEL_LEN:-32768}
      HIP_VISIBLE_DEVICES: ${HIP_VISIBLE_DEVICES:-0}
      HSA_OVERRIDE_GFX_VERSION: ${HSA_OVERRIDE_GFX_VERSION:-11.5.0}
      HF_TOKEN: ${HF_TOKEN:-}
    volumes:
      - cortex_models:/models:ro
      - vllm_cache:/root/.cache/huggingface
    shm_size: '16gb'
    networks:
      - cortex-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      <<: *healthcheck-defaults
      start_period: 180s
    deploy:
      resources:
        limits:
          memory: 48G
        reservations:
          memory: 40G

  # ===========================================================================
  # LLM Inference - llama.cpp (for SUPER_READER and GOVERNANCE)
  # ===========================================================================
  llama-cpp:
    image: ghcr.io/ggerganov/llama.cpp:server-rocm
    container_name: cortex-llama-cpp
    <<: *restart-policy
    command: >
      --model /models/gguf/nemotron-8b-instruct.Q4_K_M.gguf
      --ctx-size 131072
      --n-gpu-layers 99
      --host 0.0.0.0
      --port 8080
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render
    environment:
      HIP_VISIBLE_DEVICES: ${HIP_VISIBLE_DEVICES:-0}
      HSA_OVERRIDE_GFX_VERSION: ${HSA_OVERRIDE_GFX_VERSION:-11.5.0}
    volumes:
      - cortex_models:/models:ro
    networks:
      - cortex-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      <<: *healthcheck-defaults
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 32G
        reservations:
          memory: 16G

  # ===========================================================================
  # Workflow Automation - n8n
  # ===========================================================================
  n8n:
    image: n8nio/n8n:latest
    container_name: cortex-n8n
    <<: *restart-policy
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_USER:-admin}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD:?N8N_PASSWORD is required}
      - N8N_HOST=${CORTEX_DOMAIN:-localhost}
      - N8N_PORT=5678
      - N8N_PROTOCOL=https
      - WEBHOOK_URL=https://${CORTEX_DOMAIN:-localhost}/webhooks/
      - EXECUTIONS_DATA_SAVE_ON_SUCCESS=none
      - EXECUTIONS_DATA_SAVE_ON_ERROR=all
      - EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS=true
      - EXECUTIONS_DATA_PRUNE=true
      - EXECUTIONS_DATA_MAX_AGE=168  # 7 days
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - cortex-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:5678/healthz"]
      <<: *healthcheck-defaults

  # ===========================================================================
  # Backend - FastAPI
  # ===========================================================================
  backend:
    build:
      context: ..
      dockerfile: Dockerfile.backend
    container_name: cortex-backend
    <<: *restart-policy
    environment:
      <<: [*common-env, *model-paths]
      CORTEX_LANE_ORCHESTRATOR_URL: http://inference-vllm:8000/v1
      CORTEX_LANE_CODER_URL: http://inference-vllm:8000/v1
      CORTEX_LANE_FAST_RAG_URL: http://inference-vllm:8000/v1
      CORTEX_LANE_SUPER_READER_URL: http://llama-cpp:8080/v1
      CORTEX_LANE_GOVERNANCE_URL: http://llama-cpp:8080/v1
    volumes:
      - backend_data:/app/data
      - cortex_models:/models:ro
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    networks:
      - cortex-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/system/health"]
      <<: *healthcheck-defaults
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  # ===========================================================================
  # Frontend Builder - Build and serve React app
  # ===========================================================================
  frontend-builder:
    build:
      context: ..
      dockerfile: Dockerfile.frontend
      target: builder
    container_name: cortex-frontend-builder
    environment:
      VITE_CORTEX_API_BASE_URL: https://${CORTEX_DOMAIN:-localhost}
      VITE_API_BASE_URL: https://${CORTEX_DOMAIN:-localhost}
      NODE_ENV: production
    volumes:
      - frontend_dist:/app/dist
    command: sh -c "pnpm build && cp -r dist/* /app/dist/"

  # ===========================================================================
  # Database Migrations
  # ===========================================================================
  migrations:
    build:
      context: ..
      dockerfile: Dockerfile.backend
    container_name: cortex-migrations
    environment:
      <<: *common-env
    command: >
      sh -c "
        echo 'Waiting for PostgreSQL...' &&
        while ! pg_isready -h postgres -U cortex -d cortex; do sleep 2; done &&
        echo 'Running Alembic migrations...' &&
        cd /app && alembic upgrade head &&
        echo 'Migrations complete!'
      "
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - cortex-network
    restart: "no"

# =============================================================================
# Volumes
# =============================================================================
volumes:
  # Persistent data
  postgres_data:
    driver: local
  qdrant_data:
    driver: local
  qdrant_snapshots:
    driver: local
  n8n_data:
    driver: local
  backend_data:
    driver: local
  vllm_cache:
    driver: local
  
  # Caddy
  caddy_data:
    driver: local
  caddy_config:
    driver: local
  
  # Frontend static files
  frontend_dist:
    driver: local
  
  # Models - mount from host
  cortex_models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${MODELS_PATH:-/data/cortex-models}

# =============================================================================
# Networks
# =============================================================================
networks:
  cortex-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
