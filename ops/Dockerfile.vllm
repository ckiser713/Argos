# ROCm vLLM Dockerfile
# 
# RECOMMENDED: Use pre-built image (faster, optimized)
#   1. Load pre-built image: ./ops/load_rocm_image.sh
#   2. Update docker-compose.yml to use: image: vllm-rocm-strix:latest
#
# FALLBACK: Build from source (slower, requires network)
#   Use this Dockerfile to build from base ROCm PyTorch image

FROM rocm/pytorch:rocm7.1_ubuntu22.04_py3.11_pytorch_2.9.1

# Install build deps
RUN apt-get update && apt-get install -y git build-essential && rm -rf /var/lib/apt/lists/*

# Accept HF_TOKEN as build argument for authenticated Hugging Face downloads
ARG HF_TOKEN
ENV HF_TOKEN=${HF_TOKEN}

# Install vLLM from source (optimized for ROCm) - custom build
RUN pip uninstall -y torch torchvision || true
RUN pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/rocm7.1
# vLLM and llama.cpp provided by custom builds - assume installed or add here
RUN pip install vllm --no-build-isolation  # Update to latest or custom

# Environment for AMD
ENV HIP_VISIBLE_DEVICES=0
ENV HSA_OVERRIDE_GFX_VERSION=11.0.0 

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
CMD ["--model", "casperhansen/llama-3-70b-instruct-awq", "--gpu-memory-utilization", "0.85", "--port", "8000"]
