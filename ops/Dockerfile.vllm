FROM rocm/pytorch:rocm7.1_ubuntu22.04_py3.11_pytorch_2.9.1

# Install build deps
RUN apt-get update && apt-get install -y git build-essential

# Install vLLM from source (optimized for ROCm) - custom build
RUN pip uninstall -y torch torchvision
RUN pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/rocm7.1
# vLLM and llama.cpp provided by custom builds - assume installed or add here
RUN pip install vllm --no-build-isolation  # Update to latest or custom

# Environment for AMD
ENV HIP_VISIBLE_DEVICES=0
ENV HSA_OVERRIDE_GFX_VERSION=11.0.0 

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
CMD ["--model", "casperhansen/llama-3-70b-instruct-awq", "--gpu-memory-utilization", "0.85", "--port", "8000"]
