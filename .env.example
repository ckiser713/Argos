# =============================================================================
# Cortex Production Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values.
# For production (strix/production mode), all REQUIRED values must be set.
#
# Generate secrets with: python3 -c "import secrets; print(secrets.token_hex(32))"
# =============================================================================

# -----------------------------------------------------------------------------
# Core Environment Settings
# -----------------------------------------------------------------------------

# Environment mode: local | strix | production
# - local: SQLite database, auth disabled, for development
# - strix: PostgreSQL, auth enabled, for Strix Halo deployment
# - production: PostgreSQL, auth enabled, hardened settings
CORTEX_ENV=strix

# Authentication secret (REQUIRED for strix/production)
# Generate with: python3 -c "import secrets; print(secrets.token_hex(32))"
CORTEX_AUTH_SECRET=

# Skip authentication (only works in local mode, ignored in strix/production)
CORTEX_SKIP_AUTH=false

# Debug mode (disable in production)
DEBUG=false

# Allowed CORS origins (comma-separated)
CORTEX_ALLOWED_ORIGINS=https://your-domain.com,http://localhost:5173

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------

# PostgreSQL connection URL (auto-set for strix/production if not specified)
# Format: postgresql://user:password@host:port/database
CORTEX_DATABASE_URL=postgresql://cortex:${POSTGRES_PASSWORD}@postgres:5432/cortex

# PostgreSQL password (REQUIRED - use a strong random password)
# Generate with: python3 -c "import secrets; print(secrets.token_urlsafe(32))"
POSTGRES_PASSWORD=

# PostgreSQL settings (for docker-compose)
POSTGRES_USER=cortex
POSTGRES_DB=cortex

# SQLite paths (only used in local mode)
CORTEX_ATLAS_DB_PATH=atlas.db
CORTEX_ATLAS_CHECKPOINTS_DB_PATH=atlas_checkpoints.db

# -----------------------------------------------------------------------------
# Vector Database (Qdrant)
# -----------------------------------------------------------------------------

CORTEX_QDRANT_URL=http://qdrant:6333

# -----------------------------------------------------------------------------
# LLM Backend Configuration
# -----------------------------------------------------------------------------

# Backend type: llama_cpp | openai (vLLM uses OpenAI-compatible API)
CORTEX_LLM_BACKEND=openai

# Default LLM endpoint (vLLM server)
CORTEX_LLM_BASE_URL=http://inference-vllm:8000/v1
CORTEX_LLM_API_KEY=not-needed
CORTEX_LLM_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B

# Default lane for LLM requests
CORTEX_LLM_DEFAULT_LANE=orchestrator

# llama.cpp binary paths (for SUPER_READER and GOVERNANCE lanes)
CORTEX_LLAMA_CPP_BINARY_PATH=/usr/local/bin/llama-server
CORTEX_LLAMA_QUANTIZE_BINARY_PATH=/usr/local/bin/llama-quantize

# -----------------------------------------------------------------------------
# Model Lane Configuration
# -----------------------------------------------------------------------------

# ORCHESTRATOR Lane - Planning and agent orchestration (vLLM)
CORTEX_LANE_ORCHESTRATOR_URL=http://inference-vllm:8000/v1
CORTEX_LANE_ORCHESTRATOR_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
CORTEX_LANE_ORCHESTRATOR_MODEL_PATH=/models/vllm/orchestrator/bf16
CORTEX_LANE_ORCHESTRATOR_BACKEND=openai

# CODER Lane - Code analysis and generation (vLLM)
CORTEX_LANE_CODER_URL=http://inference-vllm:8000/v1
CORTEX_LANE_CODER_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct
CORTEX_LANE_CODER_MODEL_PATH=/models/vllm/coder/bf16
CORTEX_LANE_CODER_BACKEND=openai

# SUPER_READER Lane - Deep document ingestion (llama.cpp, 4M context)
CORTEX_LANE_SUPER_READER_URL=http://llama-super-reader:8080/v1
CORTEX_LANE_SUPER_READER_MODEL=Nemotron-8B-UltraLong-4M
CORTEX_LANE_SUPER_READER_MODEL_PATH=/models/gguf/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct-q4_k_m.gguf
CORTEX_LANE_SUPER_READER_BACKEND=llama_cpp

# FAST_RAG Lane - Quick RAG queries (vLLM)
CORTEX_LANE_FAST_RAG_URL=http://inference-vllm:8000/v1
CORTEX_LANE_FAST_RAG_MODEL=meta-llama/Llama-3.2-11B-Vision-Instruct
CORTEX_LANE_FAST_RAG_MODEL_PATH=/models/vllm/fast_rag/bf16
CORTEX_LANE_FAST_RAG_BACKEND=openai

# GOVERNANCE Lane - Compliance and policy checks (llama.cpp)
CORTEX_LANE_GOVERNANCE_URL=http://llama-governance:8081/v1
CORTEX_LANE_GOVERNANCE_MODEL=Granite-3.0-8B-Instruct
CORTEX_LANE_GOVERNANCE_MODEL_PATH=/models/gguf/granite-3.0-8b-instruct-Q4_K_M.gguf
CORTEX_LANE_GOVERNANCE_BACKEND=llama_cpp

# -----------------------------------------------------------------------------
# Mode-Specific LLM Settings
# -----------------------------------------------------------------------------

# Normal mode settings
CORTEX_NORMAL_MODE_LLM_TEMPERATURE=0.2
CORTEX_NORMAL_MODE_VALIDATION_PASSES=1
CORTEX_NORMAL_MODE_MAX_PARALLEL_TOOLS=8

# Paranoid mode settings (more validation, lower parallelism)
CORTEX_PARANOID_MODE_LLM_TEMPERATURE=0.2
CORTEX_PARANOID_MODE_VALIDATION_PASSES=2
CORTEX_PARANOID_MODE_MAX_PARALLEL_TOOLS=4

# Cortex execution mode: PARALLEL | INGEST
CORTEX_MODE=PARALLEL

# -----------------------------------------------------------------------------
# n8n Workflow Integration
# -----------------------------------------------------------------------------

CORTEX_N8N_BASE_URL=http://n8n:5678
CORTEX_N8N_API_KEY=

# n8n authentication (change in production!)
N8N_USER=admin
N8N_PASSWORD=

# n8n settings
N8N_HOST=localhost
N8N_PROTOCOL=https
WEBHOOK_URL=https://your-domain.com/webhooks/

# -----------------------------------------------------------------------------
# Hugging Face (REQUIRED for model downloads)
# -----------------------------------------------------------------------------

# Hugging Face token for gated models (Llama, etc.)
# Get from: https://huggingface.co/settings/tokens
HF_TOKEN=

# -----------------------------------------------------------------------------
# ROCm / GPU Configuration (Strix Halo specific)
# -----------------------------------------------------------------------------

# GPU device selection
HIP_VISIBLE_DEVICES=0

# ROCm GFX version override for Strix Halo (gfx1151)
# Use 11.0.0 for compatibility - matches docker-compose.strix.yml
HSA_OVERRIDE_GFX_VERSION=11.0.0

# vLLM GPU memory utilization (0.45 = ~48GB for 128GB unified memory)
# Leaves ~64GB for llama.cpp and system
VLLM_GPU_MEMORY_UTILIZATION=0.45

# Maximum model context length
VLLM_MAX_MODEL_LEN=32768

# vLLM eager mode (required for some ROCm configurations)
VLLM_ENFORCE_EAGER=true

# -----------------------------------------------------------------------------
# Frontend Configuration
# -----------------------------------------------------------------------------

# Backend API URL for frontend (used in production builds)
VITE_CORTEX_API_BASE_URL=https://your-domain.com
VITE_API_BASE_URL=https://your-domain.com

# Frontend settings
NODE_ENV=production
VITE_ENABLE_DEBUG=false
VITE_ENABLE_DEV_TOOLS=false

# -----------------------------------------------------------------------------
# SSL / Domain Configuration (Caddy)
# -----------------------------------------------------------------------------

# Your domain name (for automatic SSL via Let's Encrypt)
CORTEX_DOMAIN=your-domain.com

# Email for Let's Encrypt certificate notifications
CORTEX_ADMIN_EMAIL=admin@your-domain.com

# -----------------------------------------------------------------------------
# Monitoring & Logging (Optional)
# -----------------------------------------------------------------------------

# Log level: DEBUG | INFO | WARNING | ERROR
LOG_LEVEL=INFO

# Sentry DSN for error tracking (optional)
# SENTRY_DSN=https://xxx@sentry.io/xxx

# -----------------------------------------------------------------------------
# Backup Configuration (Optional)
# -----------------------------------------------------------------------------

# Backup retention in days
BACKUP_RETENTION_DAYS=7

# Backup storage path
BACKUP_PATH=/data/backups
